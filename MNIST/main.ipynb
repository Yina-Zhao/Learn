{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch                #导入torch库，里面包含了关于张量的各种运算\n",
    "import torchvision          #导入torchvision库，主要用于处理图像数据。里面包含了常用的数据集（如MINIST、COCO）、经典网络结构、各种关于图像处理的函数（如归一化）\n",
    "from torch.utils.data import DataLoader   \n",
    "from torch.utils.tensorboard import SummaryWriter   #tensorboard使模型训练可视化，可以看到训练过程中loss的变化曲线；summarywrite可以将训练日志写入指定文件夹下的事件文件\n",
    "from torch.utils import data\n",
    "device = \"cuda\"                 #cuda为GPU编程接口，在这里device是作为tensor或者model被分配到的位置                                                                                                                                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataLoard = torch.utils.data.DataLoader(  #定义一个装载可迭代的训练数据集的容器，以下为容器中的数据以及对其的操作 \n",
    "    torchvision.datasets.MNIST('./data/', train=True, download=True,    #确定MNIST数据集将要放置的位置, train=True表示下载的是训练集, train=False表示下载的是测试集\n",
    "                               transform=torchvision.transforms.Compose([   #transforms是对图像进行预处理的函数集，其中的compose函数作用是：将各函数联合起来发挥作用\n",
    "                                   torchvision.transforms.ToTensor(),       #ToTensor的作用是将此数据集转换成张量\n",
    "                                   torchvision.transforms.Normalize((0.456), (0.224)),  #Normalize(mean, std)为归一化函数，mean参数是数据集的平均值，std参数是数据集的标准差。通过输入这两个参数，便可以将数据归一化到我们想要的效果\n",
    "                               ])),\n",
    "    batch_size=2048, shuffle=True)      #batch_size为进行一次迭代的数据规模。shuffle()函数的作用是将数据的顺序打乱\n",
    "testDataLoard = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('./data/', train=False, download=True,\n",
    "                               transform=torchvision.transforms.Compose([\n",
    "                                   torchvision.transforms.ToTensor(),\n",
    "                                   torchvision.transforms.Normalize((0.456), (0.224))\n",
    "                               ])),\n",
    "    batch_size=1, shuffle=True)\n",
    "#以上为对数据的处理，接下来要进行网络结构的构造"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class zyn(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(zyn, self).__init__()\n",
    "        self.conv0 = torch.nn.Conv2d(1, 10, (3, 3), 1, 1)           \n",
    "        self.conv1 = torch.nn.Conv2d(10, 10, (3, 3), 1, 1)\n",
    "        self.conv2 = torch.nn.Conv2d(10, 10, (3, 3), 1, 1)\n",
    "        self.conv3 = torch.nn.Conv2d(10, 10, (3, 3), 1, 1)\n",
    "        self.conv4 = torch.nn.Conv2d(10, 10, (3, 3), 1, 1)\n",
    "        self.conv5 = torch.nn.Conv2d(10, 10, (3, 3), 1, 1)\n",
    "        self.conv6 = torch.nn.Conv2d(10, 10, (3, 3), 1, 1)\n",
    "        self.conv7 = torch.nn.Conv2d(10, 10, (3, 3), 1, 1)\n",
    "        self.conv8 = torch.nn.Conv2d(10, 10, (3, 3), 1, 1)\n",
    "        '''\n",
    "        现在开始构建网络结构。\n",
    "        conv2d表示该卷积层处理的数据是二维的。\n",
    "        nn.Conv2d中的参数介绍:\n",
    "        第一位表示输入层神经元的个数,第二位为输出层神经元的个数,第三位为卷积核的size,第四位为卷积核的步长,第五位为padding,默认为0,第六位为padding_mode(填充方式,默认为常数填充)\n",
    "        这里一共有8层卷积层,原始数据输入层不算作神经网络的层数\n",
    "        '''                 \n",
    "        self.f = torch.nn.Flatten()    #Flatten(dim)的作用是降维，dim表示从第dim个维度展开，将后面的维度降为一维。这个函数常用在卷积层到全连接层的过度。\n",
    "        self.connected = torch.nn.Linear(7840, 10, True) \n",
    "        '''\n",
    "        Linear为全连接层,一般放在卷积层的后面。这一层相当于将之前提取出的特征值组合起来。\n",
    "        各参数的含义:\n",
    "        第一个为输入神经元的个数。由于全连接之前的flatten函数已经将数据展开了,所以输入神经元的个数为(28 * 28 * 10),\n",
    "        28 * 28是图像的大小,因为是灰度图,所以还有个乘1省略了,10则表示铺平前该层的神经元个数.\n",
    "        需要注意的是, 由于我选择填充了一层, 所以经过三次卷积后图片尺寸未发生改变.\n",
    "        若不选择填充, 这里填入的应该是(22 * 22 * 10).\n",
    "        第二个参数表示输出神经元的个数,因为该程序是一个十分类的问题(0 ~ 9),所以输出神经元的个数应为10.\n",
    "        第三个参数表示bias(偏置参数,起到微调的作用),True表示希望添加这样一个偏置值,否则填False.\n",
    "\n",
    "        '''\n",
    "        self.relu = torch.nn.ReLU(True)     #True表示希望ReLu对输出的数据进行改变\n",
    "        # 定义向前传播的过程\n",
    "    def forward(self, X):\n",
    "        X = self.conv0(X)  \n",
    "        X = self.conv1(X)\n",
    "        X = self.conv2(X)\n",
    "        X = self.relu(X)\n",
    "        X = self.f(X)\n",
    "        X = self.connected(X)  \n",
    "        X = self.relu(X)             #向前传播的过程\n",
    "        return X\n",
    "\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = zyn()         #实例化上面定义的网络结构\n",
    "net.to(device)      #指定网络结构运行的地方\n",
    "opt = torch.optim.Adam(net.parameters(), 0.002)  \n",
    "'''\n",
    "optim意为最好的。这里使用Adam算法对网络进行优化。\n",
    "Adam里面的参数介绍:\n",
    "第一个参数为params(翻译成中文也是参数的意思),这里填入net.parameters()相当于填入了一个优化器,将返回来的参数进行优化后再将其填入params的位置.\n",
    "第二个参数是lr(learn rate),默认为0.001,学习率过大可能会出现梯度爆炸从而错过最低点,过小可能会出现梯度消失的现象,导致学习的速率很慢.\n",
    "第三个参数是betas = (0.9, 0.999),通常被用于正则化,防止模型过拟合.而正则化的作用为减少不那么重要的特征变量的数量. 所以beta用于调整L1或L2正则化项的权重.\n",
    "第四个参数是eps,默认值为1e-8,是加到分母里面的项,用于防止计算时出现除以0的错误,保证数值的稳定性.\n",
    "第五个参数是weight_decay(默认为0), 权重衰减, 是L2正则化项的一个系数.\n",
    "\n",
    "'''\n",
    "loss = torch.nn.CrossEntropyLoss()  #交叉熵损失函数，常用于多分类的网络模型中。用于衡量实际输出与预期输出的距离。这里相当于给原本的函数换个名字。\n",
    "loss.to(device)         #指定损失函数运行的地方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2966256141662598\n",
      "2.1384899616241455\n",
      "1.8355185985565186\n",
      "1.5128458738327026\n",
      "1.1157293319702148\n",
      "0.7949536442756653\n",
      "0.5668877363204956\n",
      "0.4767947793006897\n",
      "0.4585520625114441\n",
      "0.4241270124912262\n",
      "0.3954535126686096\n",
      "0.4243759512901306\n",
      "0.42235198616981506\n",
      "0.31705835461616516\n",
      "0.32847267389297485\n",
      "0.27531400322914124\n",
      "0.3032604455947876\n",
      "0.31686216592788696\n",
      "0.31460291147232056\n",
      "0.2783046364784241\n",
      "0.2599778175354004\n",
      "0.2685944437980652\n",
      "0.21238401532173157\n",
      "0.21999524533748627\n",
      "0.2302144169807434\n",
      "0.22167041897773743\n",
      "0.19857844710350037\n",
      "0.21444308757781982\n",
      "0.21293413639068604\n",
      "0.205055832862854\n",
      "0.16719050705432892\n",
      "0.1965045928955078\n",
      "0.16155767440795898\n",
      "0.19544990360736847\n",
      "0.15338554978370667\n",
      "0.15163710713386536\n",
      "0.11737067997455597\n",
      "0.16304028034210205\n",
      "0.1324148178100586\n",
      "0.1339157521724701\n",
      "0.1451118290424347\n",
      "0.1459941267967224\n",
      "0.12013877928256989\n",
      "0.14679329097270966\n",
      "0.13150504231452942\n",
      "0.1375003457069397\n",
      "0.1331150233745575\n",
      "0.13572552800178528\n",
      "0.13917915523052216\n",
      "0.10383671522140503\n",
      "0.12125859409570694\n",
      "0.12180072069168091\n",
      "0.13289287686347961\n",
      "0.11576531827449799\n",
      "0.11862719804048538\n",
      "0.12367038428783417\n",
      "0.10953804105520248\n",
      "0.12583225965499878\n",
      "0.11741320043802261\n",
      "0.09913971275091171\n",
      "0.09513038396835327\n",
      "0.09149569272994995\n",
      "0.10043154656887054\n",
      "0.10265066474676132\n",
      "0.10035711526870728\n",
      "0.09883096069097519\n",
      "0.08623109757900238\n",
      "0.0960809737443924\n",
      "0.0939364954829216\n",
      "0.09349416196346283\n",
      "0.10873765498399734\n",
      "0.09466240555047989\n",
      "0.07293109595775604\n",
      "0.09256492555141449\n",
      "0.09148845821619034\n",
      "0.10042983293533325\n",
      "0.10546936839818954\n",
      "0.09881001710891724\n",
      "0.07543954253196716\n",
      "0.08052292466163635\n",
      "0.08550748229026794\n",
      "0.07180599123239517\n",
      "0.08316878974437714\n",
      "0.07987160235643387\n",
      "0.0800669714808464\n",
      "0.07751420140266418\n",
      "0.08081258088350296\n",
      "0.06321526318788528\n",
      "0.0813073068857193\n",
      "0.09968262165784836\n",
      "0.0720914751291275\n",
      "0.07573260366916656\n",
      "0.06260834634304047\n",
      "0.08131372928619385\n",
      "0.05926337465643883\n",
      "0.06946781277656555\n",
      "0.06587223708629608\n",
      "0.05497701093554497\n",
      "0.07686273753643036\n",
      "0.06895594298839569\n",
      "0.09513545036315918\n",
      "0.05663016438484192\n",
      "0.07026058435440063\n",
      "0.06495341658592224\n",
      "0.06518000364303589\n",
      "0.05241905897855759\n",
      "0.08542796224355698\n",
      "0.06689642369747162\n",
      "0.058907490223646164\n",
      "0.07856149971485138\n",
      "0.05817663297057152\n",
      "0.06316625326871872\n",
      "0.0663890540599823\n",
      "0.06181465461850166\n",
      "0.04413692653179169\n",
      "0.07068569958209991\n",
      "0.04725862294435501\n",
      "0.05529667064547539\n",
      "0.07463729381561279\n",
      "0.06715951859951019\n",
      "0.06080798804759979\n",
      "0.05755038559436798\n",
      "0.0505455918610096\n",
      "0.04896474629640579\n",
      "0.04591379314661026\n",
      "0.05078486353158951\n",
      "0.04518069699406624\n",
      "0.0519944429397583\n",
      "0.06738181412220001\n",
      "0.06291543692350388\n",
      "0.06052717566490173\n",
      "0.0458863228559494\n",
      "0.04329371824860573\n",
      "0.0379042774438858\n",
      "0.04249628633260727\n",
      "0.0594758614897728\n",
      "0.04867932200431824\n",
      "0.06814849376678467\n",
      "0.0654531791806221\n",
      "0.0698472112417221\n",
      "0.04363191872835159\n",
      "0.05397382378578186\n",
      "0.06090585142374039\n",
      "0.0415353961288929\n",
      "0.0539010614156723\n",
      "0.04307980835437775\n",
      "0.05572514981031418\n",
      "0.048417381942272186\n",
      "0.05767826363444328\n",
      "0.049785174429416656\n",
      "0.05372804403305054\n",
      "0.03284420073032379\n",
      "0.0555792897939682\n",
      "0.05544604733586311\n",
      "0.03644638508558273\n",
      "0.04733077809214592\n",
      "0.04180765897035599\n",
      "0.04483259096741676\n",
      "0.034953486174345016\n",
      "0.039060842245817184\n",
      "0.037404097616672516\n",
      "0.05581787973642349\n",
      "0.05785111337900162\n",
      "0.05020435154438019\n",
      "0.038724809885025024\n",
      "0.033815883100032806\n",
      "0.04768763482570648\n",
      "0.04164106771349907\n",
      "0.03952259570360184\n",
      "0.04225064069032669\n",
      "0.05122733861207962\n",
      "0.04627811908721924\n",
      "0.0380447581410408\n",
      "0.055027108639478683\n",
      "0.03832688555121422\n",
      "0.033721644431352615\n",
      "0.04009488970041275\n",
      "0.043340153992176056\n",
      "0.04463736712932587\n",
      "0.028322966769337654\n",
      "0.0376054048538208\n",
      "0.04240149259567261\n",
      "0.02932695671916008\n",
      "0.03228030353784561\n",
      "0.025245841592550278\n",
      "0.041420213878154755\n",
      "0.048555172979831696\n",
      "0.033526044338941574\n",
      "0.03437384217977524\n",
      "0.04125376418232918\n",
      "0.035375870764255524\n",
      "0.039828065782785416\n",
      "0.03360412269830704\n",
      "0.0377177819609642\n",
      "0.03833464905619621\n",
      "0.02672259509563446\n",
      "0.03584325313568115\n",
      "0.03923996910452843\n",
      "0.039338745176792145\n",
      "0.04789457097649574\n",
      "0.034139201045036316\n",
      "0.03683694452047348\n",
      "0.03518323227763176\n",
      "0.03466518595814705\n",
      "0.044475141912698746\n",
      "0.030337287113070488\n",
      "0.03281347081065178\n",
      "0.04182181879878044\n",
      "0.02700192853808403\n",
      "0.026262985542416573\n",
      "0.03318517282605171\n",
      "0.03773350268602371\n",
      "0.024097099900245667\n",
      "0.030013225972652435\n",
      "0.024858979508280754\n",
      "0.031941935420036316\n",
      "0.023034226149320602\n",
      "0.03498858958482742\n",
      "0.03185495734214783\n",
      "0.04385337978601456\n",
      "0.03691525757312775\n",
      "0.0307941772043705\n",
      "0.031043928116559982\n",
      "0.027066051959991455\n",
      "0.037220537662506104\n",
      "0.044949114322662354\n",
      "0.0379817821085453\n",
      "0.028020311146974564\n",
      "0.03129252418875694\n",
      "0.03930561617016792\n",
      "0.03382154554128647\n",
      "0.03398330509662628\n",
      "0.03204857558012009\n",
      "0.037433892488479614\n",
      "0.030932236462831497\n",
      "0.025738127529621124\n",
      "0.038631461560726166\n",
      "0.04167822748422623\n",
      "0.04270624369382858\n",
      "0.026861295104026794\n",
      "0.018085969612002373\n",
      "0.030392877757549286\n",
      "0.02747928723692894\n",
      "0.016008906066417694\n",
      "0.026392977684736252\n",
      "0.01981160044670105\n",
      "0.027671631425619125\n",
      "0.02948257327079773\n",
      "0.026359014213085175\n",
      "0.03596639633178711\n",
      "0.021482929587364197\n",
      "0.02533404529094696\n",
      "0.01795697584748268\n",
      "0.03402499482035637\n",
      "0.021493980661034584\n",
      "0.032652370631694794\n",
      "0.022010598331689835\n",
      "0.026837041601538658\n",
      "0.0292085949331522\n",
      "0.027833478525280952\n",
      "0.023671846836805344\n",
      "0.023570556193590164\n",
      "0.026417069137096405\n",
      "0.03921312838792801\n",
      "0.031589582562446594\n",
      "0.02665719762444496\n",
      "0.03693030774593353\n",
      "0.030127115547657013\n",
      "0.03806959465146065\n",
      "0.021361740306019783\n",
      "0.029905878007411957\n",
      "0.023215608671307564\n",
      "0.021675018593668938\n",
      "0.021877821534872055\n",
      "0.019269047304987907\n",
      "0.014832958579063416\n",
      "0.030379518866539\n",
      "0.021127458661794662\n",
      "0.016479749232530594\n",
      "0.015506081283092499\n",
      "0.02864438109099865\n",
      "0.02285708114504814\n",
      "0.01818489469587803\n",
      "0.021640917286276817\n",
      "0.021259590983390808\n",
      "0.025436516851186752\n",
      "0.03032698854804039\n",
      "0.025886397808790207\n",
      "0.02054542489349842\n",
      "0.02860618196427822\n",
      "0.02899155206978321\n",
      "0.01446451060473919\n",
      "0.02273080311715603\n",
      "0.02043760195374489\n",
      "0.02841075137257576\n",
      "0.017288925126194954\n",
      "0.029494810849428177\n",
      "0.03614281490445137\n",
      "0.030770335346460342\n",
      "0.028741538524627686\n",
      "0.029189929366111755\n",
      "0.020379845052957535\n",
      "0.0153434406965971\n",
      "0.02395099401473999\n",
      "0.019503697752952576\n",
      "0.02015363797545433\n",
      "0.016588270664215088\n",
      "0.018386676907539368\n",
      "0.025497518479824066\n",
      "0.023214982822537422\n",
      "0.02086593210697174\n",
      "0.021154720336198807\n",
      "0.025153087452054024\n",
      "0.020376745611429214\n",
      "0.017226766794919968\n",
      "0.020144902169704437\n",
      "0.030139971524477005\n",
      "0.017782341688871384\n",
      "0.029247738420963287\n",
      "0.020535770803689957\n",
      "0.022001812234520912\n",
      "0.01965966820716858\n",
      "0.027096373960375786\n",
      "0.03135933727025986\n",
      "0.019196029752492905\n",
      "0.010449817404150963\n",
      "0.02172108367085457\n",
      "0.021617740392684937\n",
      "0.029084589332342148\n",
      "0.021586641669273376\n",
      "0.011993808671832085\n",
      "0.013005176559090614\n",
      "0.01792692393064499\n",
      "0.012284686788916588\n",
      "0.017306452617049217\n",
      "0.021297693252563477\n",
      "0.022955475375056267\n",
      "0.012240445241332054\n",
      "0.02328503504395485\n",
      "0.025053158402442932\n",
      "0.010269975289702415\n",
      "0.012239653617143631\n",
      "0.03462342172861099\n",
      "0.029952144250273705\n",
      "0.016991877928376198\n",
      "0.028749307617545128\n",
      "0.01649124175310135\n",
      "0.023834941908717155\n",
      "0.026301980018615723\n",
      "0.024248361587524414\n",
      "0.021093148738145828\n",
      "0.02049439400434494\n",
      "0.02147991769015789\n",
      "0.023170236498117447\n",
      "0.015449789352715015\n",
      "0.019894754514098167\n",
      "0.02478044107556343\n",
      "0.02695516124367714\n",
      "0.024278778582811356\n",
      "0.03449379652738571\n",
      "0.014887027442455292\n",
      "0.017372136935591698\n",
      "0.014768450520932674\n",
      "0.019290635362267494\n",
      "0.014452783390879631\n",
      "0.023372478783130646\n",
      "0.01679641380906105\n",
      "0.026755232363939285\n",
      "0.013577722012996674\n",
      "0.012737213633954525\n",
      "0.016838664188981056\n",
      "0.0163717120885849\n",
      "0.025574684143066406\n",
      "0.021071134135127068\n",
      "0.029378630220890045\n",
      "0.02262083813548088\n",
      "0.017821867018938065\n",
      "0.013164488598704338\n",
      "0.019632752984762192\n",
      "0.02149929292500019\n",
      "0.02941780909895897\n",
      "0.018736492842435837\n",
      "0.018743697553873062\n",
      "0.025535136461257935\n",
      "0.019260775297880173\n",
      "0.01380513608455658\n",
      "0.018314659595489502\n",
      "0.012959863059222698\n",
      "0.01867479830980301\n",
      "0.01613914780318737\n",
      "0.013290774077177048\n",
      "0.015811331570148468\n",
      "0.014783935621380806\n",
      "0.01124487817287445\n",
      "0.013288306072354317\n",
      "0.010571721009910107\n",
      "0.01209873054176569\n",
      "0.01431741751730442\n",
      "0.013940950855612755\n",
      "0.01599287800490856\n",
      "0.013760779052972794\n",
      "0.017657477408647537\n",
      "0.010163703002035618\n",
      "0.013395518064498901\n",
      "0.014498668722808361\n",
      "0.011773290112614632\n",
      "0.01603558659553528\n",
      "0.013687698170542717\n",
      "0.016742708161473274\n",
      "0.013913752511143684\n",
      "0.01720847561955452\n",
      "0.01647206023335457\n",
      "0.012432805262506008\n",
      "0.015473101288080215\n",
      "0.018779639154672623\n",
      "0.015652384608983994\n",
      "0.017704114317893982\n",
      "0.008237464353442192\n",
      "0.02193417400121689\n",
      "0.027918433770537376\n",
      "0.008477294817566872\n",
      "0.011166618205606937\n",
      "0.010329127311706543\n",
      "0.011710538528859615\n",
      "0.015079844743013382\n",
      "0.014168131165206432\n",
      "0.010026701726019382\n",
      "0.012854943051934242\n",
      "0.011879636906087399\n",
      "0.014123750850558281\n",
      "0.016533471643924713\n",
      "0.017299015074968338\n",
      "0.016796452924609184\n",
      "0.01377091184258461\n",
      "0.008077766746282578\n",
      "0.015835020691156387\n",
      "0.023031778633594513\n",
      "0.010428028181195259\n",
      "0.010002548806369305\n",
      "0.013020401820540428\n",
      "0.025226788595318794\n",
      "0.015569060109555721\n",
      "0.013786112889647484\n",
      "0.010778186842799187\n",
      "0.013709444552659988\n",
      "0.015085343271493912\n",
      "0.01466684602200985\n",
      "0.011309541761875153\n",
      "0.009440865367650986\n",
      "0.008160734549164772\n",
      "0.010857753455638885\n",
      "0.01212329138070345\n",
      "0.013747947290539742\n",
      "0.008078573271632195\n",
      "0.008026591502130032\n",
      "0.008026070892810822\n",
      "0.010730195790529251\n",
      "0.010007893666625023\n",
      "0.006341858766973019\n",
      "0.010712781921029091\n",
      "0.007622523698955774\n",
      "0.014681637287139893\n",
      "0.0098368339240551\n",
      "0.008365454152226448\n",
      "0.007692570798099041\n",
      "0.010045977309346199\n",
      "0.011702245101332664\n",
      "0.009631825610995293\n",
      "0.015322612598538399\n",
      "0.009469220414757729\n",
      "0.00991029106080532\n",
      "0.011348584666848183\n",
      "0.008322447538375854\n",
      "0.008231455460190773\n",
      "0.007542738225311041\n",
      "0.006821466144174337\n",
      "0.022040674462914467\n",
      "0.012010132893919945\n",
      "0.010730110108852386\n",
      "0.017830874770879745\n",
      "0.007856078445911407\n",
      "0.007572080008685589\n",
      "0.005539930425584316\n",
      "0.011508602648973465\n",
      "0.007524264045059681\n",
      "0.007593922782689333\n",
      "0.00825815461575985\n",
      "0.006619664840400219\n",
      "0.007982229813933372\n",
      "0.0037564868107438087\n",
      "0.008910346776247025\n",
      "0.011315133422613144\n",
      "0.005955015774816275\n",
      "0.007204634603112936\n",
      "0.011159932240843773\n",
      "0.004621581174433231\n",
      "0.006314790807664394\n",
      "0.006701773032546043\n",
      "0.013774735853075981\n",
      "0.009345044381916523\n",
      "0.004137365147471428\n",
      "0.008109505288302898\n",
      "0.009498195722699165\n",
      "0.009424099698662758\n",
      "0.007980521768331528\n",
      "0.018694017082452774\n",
      "0.01157026644796133\n",
      "0.006600833497941494\n",
      "0.010556619614362717\n",
      "0.008197139948606491\n",
      "0.0077632418833673\n",
      "0.007034766022115946\n",
      "0.0040510171093046665\n",
      "0.007638934068381786\n",
      "0.0088995061814785\n",
      "0.008180078119039536\n",
      "0.0071824295446276665\n",
      "0.01017896831035614\n",
      "0.006347758695483208\n",
      "0.008015886880457401\n",
      "0.0065988400019705296\n",
      "0.005796030163764954\n",
      "0.006089141126722097\n",
      "0.0077892085537314415\n",
      "0.008469494059681892\n",
      "0.00734504871070385\n",
      "0.005678021349012852\n",
      "0.005570140667259693\n",
      "0.014169656671583652\n",
      "0.008160987868905067\n",
      "0.006142854690551758\n",
      "0.007563706953078508\n",
      "0.013439172878861427\n",
      "0.008560744114220142\n",
      "0.008712757378816605\n",
      "0.01231984794139862\n",
      "0.009066984988749027\n",
      "0.005050136707723141\n",
      "0.009724544361233711\n",
      "0.007241456303745508\n",
      "0.0049489159137010574\n",
      "0.005617833696305752\n",
      "0.00617595948278904\n",
      "0.006669468246400356\n",
      "0.008662624284625053\n",
      "0.005808890797197819\n",
      "0.0068354434333741665\n",
      "0.004781981930136681\n",
      "0.00653870590031147\n",
      "0.002970779314637184\n",
      "0.004890613257884979\n",
      "0.006611294113099575\n",
      "0.003265908919274807\n",
      "0.004433568567037582\n",
      "0.007485911715775728\n",
      "0.0037512467242777348\n",
      "0.012305250391364098\n",
      "0.006983438041061163\n",
      "0.005258817225694656\n",
      "0.004241844639182091\n",
      "0.006755714304745197\n",
      "0.006669533438980579\n",
      "0.004953755531460047\n",
      "0.007155496161431074\n",
      "0.005618596449494362\n",
      "0.00879848375916481\n",
      "0.0060437871143221855\n",
      "0.010285520926117897\n",
      "0.005215504206717014\n",
      "0.0033971911761909723\n",
      "0.0059522781521081924\n",
      "0.006809443701058626\n",
      "0.007833738811314106\n",
      "0.002307836664840579\n",
      "0.004834365099668503\n",
      "0.010955357924103737\n",
      "0.008000075817108154\n",
      "0.006433703470975161\n",
      "0.006495277397334576\n",
      "0.003146266797557473\n",
      "0.0033886772580444813\n",
      "0.005264647305011749\n",
      "0.005272229667752981\n",
      "0.0033514327369630337\n",
      "0.006455142050981522\n",
      "0.0017902477411553264\n",
      "0.004817291162908077\n",
      "0.004467498976737261\n",
      "0.006289423443377018\n",
      "0.00513183418661356\n",
      "0.004464590921998024\n",
      "0.005042345728725195\n",
      "0.00808742269873619\n",
      "0.006615028716623783\n",
      "0.0032198401167988777\n",
      "0.004988996312022209\n",
      "0.005656777881085873\n",
      "0.011805178597569466\n",
      "0.006470243446528912\n",
      "0.005126097705215216\n",
      "0.004433479160070419\n",
      "0.003955055959522724\n",
      "0.003353656269609928\n",
      "0.0014059003442525864\n",
      "0.005265442188829184\n",
      "0.005107744596898556\n",
      "0.004254836123436689\n",
      "0.007095444947481155\n",
      "0.007073318585753441\n",
      "0.003134107915684581\n",
      "0.003888797014951706\n",
      "0.0028618359938263893\n",
      "0.005666308104991913\n",
      "0.002763560973107815\n",
      "0.003413642290979624\n",
      "0.00413844408467412\n",
      "0.004470198415219784\n",
      "0.004058195278048515\n",
      "0.004966254346072674\n",
      "0.0039436593651771545\n",
      "0.00426988210529089\n",
      "0.002432728884741664\n",
      "0.005941065028309822\n",
      "0.007586232852190733\n",
      "0.0032922965474426746\n",
      "0.004068242851644754\n",
      "0.0031595276668667793\n",
      "0.003951076418161392\n",
      "0.005131735000759363\n",
      "0.0026564663276076317\n",
      "0.001048979233019054\n",
      "0.002550186589360237\n",
      "0.0053516291081905365\n",
      "0.0018401446286588907\n",
      "0.0020438581705093384\n",
      "0.0023755182046443224\n",
      "0.004715004004538059\n",
      "0.002626976231113076\n",
      "0.004643280524760485\n",
      "0.002921292558312416\n",
      "0.002593840239569545\n",
      "0.0033209442626684904\n",
      "0.0027966799680143595\n",
      "0.004258790984749794\n",
      "0.0023447375278919935\n",
      "0.00339493528008461\n",
      "0.004873708821833134\n",
      "0.004492570646107197\n",
      "0.0023580442648380995\n",
      "0.0018887808546423912\n",
      "0.0023745775688439608\n",
      "0.0019068662077188492\n",
      "0.005168572533875704\n",
      "0.0018522334285080433\n",
      "0.0038606072776019573\n",
      "0.0021991112735122442\n",
      "0.0045145172625780106\n",
      "0.004480293020606041\n",
      "0.0038849664852023125\n",
      "0.0011906425934284925\n",
      "0.007544005289673805\n",
      "0.0027734413743019104\n",
      "0.0019186537247151136\n",
      "0.002202226547524333\n",
      "0.002496207831427455\n",
      "0.0035849816631525755\n",
      "0.005065522156655788\n",
      "0.001986830262467265\n",
      "0.003699979744851589\n",
      "0.00413739075884223\n",
      "0.0022345450706779957\n",
      "0.004369974136352539\n",
      "0.004549182951450348\n",
      "0.004332880489528179\n",
      "0.003759837243705988\n",
      "0.0024281139485538006\n",
      "0.002748204628005624\n",
      "0.0034116378519684076\n",
      "0.0018059355206787586\n",
      "0.0010912553407251835\n",
      "0.0023039551451802254\n",
      "0.006372961215674877\n",
      "0.007113796658813953\n",
      "0.0032581472769379616\n",
      "0.0031262387055903673\n",
      "0.0044728550128638744\n",
      "0.006575495935976505\n",
      "0.002165272831916809\n",
      "0.002075495198369026\n",
      "0.0017718604067340493\n",
      "0.0032455897890031338\n",
      "0.006978347431868315\n",
      "0.004461457021534443\n",
      "0.0012292349711060524\n",
      "0.002697121351957321\n",
      "0.0014005678240209818\n",
      "0.003559777745977044\n",
      "0.0014095313381403685\n",
      "0.004458886571228504\n",
      "0.003347806166857481\n",
      "0.004515687469393015\n",
      "0.004088692832738161\n",
      "0.0016815674025565386\n",
      "0.003434635465964675\n",
      "0.0023632312659174204\n",
      "0.005011278670281172\n",
      "0.0034506996162235737\n",
      "0.002050666604191065\n",
      "0.00796070322394371\n",
      "0.004702841863036156\n",
      "0.0027501420117914677\n",
      "0.002585717709735036\n",
      "0.0038053798489272594\n",
      "0.004931941628456116\n",
      "0.004150870256125927\n",
      "0.006681143306195736\n",
      "0.007589041255414486\n",
      "0.008389701135456562\n",
      "0.0074026561342179775\n",
      "0.007732948288321495\n",
      "0.006862244568765163\n",
      "0.005127457901835442\n",
      "0.016816619783639908\n",
      "0.009511468932032585\n",
      "0.008203030563890934\n",
      "0.00671237101778388\n",
      "0.004654727876186371\n",
      "0.018733225762844086\n",
      "0.005742634646594524\n",
      "0.006819689646363258\n",
      "0.016448762267827988\n",
      "0.006605800241231918\n",
      "0.012235124595463276\n",
      "0.008278904482722282\n",
      "0.004121408797800541\n",
      "0.014530623331665993\n",
      "0.010192401707172394\n",
      "0.006730756722390652\n",
      "0.01602018252015114\n",
      "0.006859598681330681\n",
      "0.011608253233134747\n",
      "0.010996706783771515\n",
      "0.006675470620393753\n",
      "0.010713888332247734\n",
      "0.007418816909193993\n",
      "0.0133808683604002\n",
      "0.007915262132883072\n",
      "0.0035528945736587048\n",
      "0.01144663617014885\n",
      "0.008326685056090355\n"
     ]
    }
   ],
   "source": [
    "#迭代开始\n",
    "for epoch in range(25):\n",
    "    '''\n",
    "    epoch为全部数据输入后进行了一次向前传播和向后传播的过程.\n",
    "    iteration(迭代)为batch_size的batch进行一次向前传播和向后传播的过程.\n",
    "    所以假如一共有1000个数据, batch_size = 100, 那么epoch = 10(iteration).\n",
    "    如果数据不能整除batch_size, 那么可以自己设置是否要将除不断的部分丢弃不进行迭代.\n",
    "    比如:一共有1007个数据, batch_size = 100, 那么还剩7个, 如果丢弃, epoch = 10(iteration), 否则epoch = 11(iteration).\n",
    "    在这里, 我将全部数据的迭代次数设置为25次.\n",
    "    '''\n",
    "    for image, label in trainDataLoard:     #对所有数据进行一次遍历\n",
    "        opt.zero_grad()                  #zero_grad()的作用是将梯度清零. 因为各个mini_batch之间的梯度并不需要混合运算\n",
    "        image = image.to(device)            #将image和label复制到设定的device上运算\n",
    "        label = label.to(device)\n",
    "        label = torch.nn.functional.one_hot(label).float()\n",
    "        '''\n",
    "        将label转换成one-hot编码.\n",
    "        one-hot函数的参数介绍:\n",
    "        第一个参数是需要转换的标签, 在本程序中做的是手写数字识别, 所以标签为 0 ~ 9 这10个数字.(注意填入的是张量).\n",
    "        第二个参数是总的类别数(相当于one-hot编码的列数), 在这个程序中一用有10个数字需要识别, 所以类别数为10.(注意, 这个参数是int型).\n",
    "        举个简单的例子:\n",
    "        假如我们要识别的是(\"cat\", \"dog\", \"pig\"), 那么num_class=3, 转化成one-hot编码为以下结果:\n",
    "        tensor(\n",
    "        [1, 0, 0],\n",
    "        [0, 1, 0],\n",
    "        [0, 0, 1]\n",
    "        ),\n",
    "        如果我们没有指定num_class的值, 那么one-hot的列数为标签中的(max + 1).(因为从0到max一共有(max + 1)个数)\n",
    "        比如, 我们输入的标签是tensor([1, 2]), 并且没有指定num_class, 那么转换成one-hot编码后将得到如下结果:\n",
    "        tensor(\n",
    "        [0, 1, 0],\n",
    "        [0, 0, 1],\n",
    "        )\n",
    "        事实上, 在我们下载的数据集里面已经包含了各标签, 所以并不需要我们手动设置需要转换的类别和类别数, 否则可能会有维度不匹配的报错.\n",
    "        '''\n",
    "        out = net(image)        #这里就将数据输入进了之前定义好的网络结构中了, 并将输出的数据赋给变量out.\n",
    "        Loss = loss(out, label)\n",
    "        '''\n",
    "        上一个语句中数据已经向前传播过一次了, 所以这里计算输入数据的损失函数.\n",
    "        loss函数(前面已经提到这是torch.nn.CrossEntropyLoss()函数的别名)的参数介绍;\n",
    "        第一个参数是实际输出的数据, 第二个参数是预期得到的数据.\n",
    "        这里需要注意的是, 根据定义, loss函数是指单个数据的实际输出与预期数据之间的距离, cost函数是指一组数据的实际输出与预期数据的距离.(就是将该组数据中各loss函数相加, 然后求平均值)\n",
    "        而在这里的loss函数, 其实已经将mini_batch中所有的损失函数相加求了平均值并赋值给了我们定义的变量, 所以这里的loss函数其实是相当于mini_batch的成本函数.\n",
    "        其实在torch.nn.CrossEntropyLoss()中有一个默认参数为reduction = 'mean'(将各损失函数相加求平均值), 我们也可以手动填入reduction = 'sum', 这样输出的便是损失函数的和.\n",
    "        并且, 交叉熵损失函数里面包含了softmax(), 所以在设置向前传播的过程中, 可以不加上这个softmax函数.\n",
    "        '''\n",
    "        print(float(Loss))\n",
    "        Loss.backward()     #进行向后传播\n",
    "        opt.step()          #更新权重\n",
    "        #训练集训练结束\n",
    "torch.save(zyn, 'net.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5])   tensor([[3]], device='cuda:0')\n",
      "tensor([2])   tensor([[7]], device='cuda:0')\n",
      "tensor([3])   tensor([[7]], device='cuda:0')\n",
      "tensor([3])   tensor([[9]], device='cuda:0')\n",
      "tensor([8])   tensor([[3]], device='cuda:0')\n",
      "tensor([3])   tensor([[5]], device='cuda:0')\n",
      "tensor([1])   tensor([[3]], device='cuda:0')\n",
      "tensor([4])   tensor([[9]], device='cuda:0')\n",
      "tensor([6])   tensor([[5]], device='cuda:0')\n",
      "tensor([9])   tensor([[3]], device='cuda:0')\n",
      "tensor([0])   tensor([[8]], device='cuda:0')\n",
      "tensor([5])   tensor([[3]], device='cuda:0')\n",
      "tensor([2])   tensor([[0]], device='cuda:0')\n",
      "tensor([6])   tensor([[0]], device='cuda:0')\n",
      "tensor([2])   tensor([[1]], device='cuda:0')\n",
      "tensor([2])   tensor([[1]], device='cuda:0')\n",
      "tensor([5])   tensor([[3]], device='cuda:0')\n",
      "tensor([3])   tensor([[5]], device='cuda:0')\n",
      "tensor([9])   tensor([[4]], device='cuda:0')\n",
      "tensor([4])   tensor([[2]], device='cuda:0')\n",
      "tensor([2])   tensor([[4]], device='cuda:0')\n",
      "tensor([5])   tensor([[0]], device='cuda:0')\n",
      "tensor([9])   tensor([[0]], device='cuda:0')\n",
      "tensor([2])   tensor([[8]], device='cuda:0')\n",
      "tensor([7])   tensor([[3]], device='cuda:0')\n",
      "tensor([7])   tensor([[3]], device='cuda:0')\n",
      "tensor([2])   tensor([[3]], device='cuda:0')\n",
      "tensor([3])   tensor([[5]], device='cuda:0')\n",
      "tensor([9])   tensor([[5]], device='cuda:0')\n",
      "tensor([5])   tensor([[3]], device='cuda:0')\n",
      "tensor([6])   tensor([[0]], device='cuda:0')\n",
      "tensor([6])   tensor([[0]], device='cuda:0')\n",
      "tensor([2])   tensor([[0]], device='cuda:0')\n",
      "tensor([5])   tensor([[2]], device='cuda:0')\n",
      "tensor([2])   tensor([[3]], device='cuda:0')\n",
      "tensor([4])   tensor([[9]], device='cuda:0')\n",
      "tensor([2])   tensor([[6]], device='cuda:0')\n",
      "tensor([3])   tensor([[9]], device='cuda:0')\n",
      "tensor([3])   tensor([[5]], device='cuda:0')\n",
      "tensor([9])   tensor([[1]], device='cuda:0')\n",
      "tensor([6])   tensor([[8]], device='cuda:0')\n",
      "tensor([1])   tensor([[2]], device='cuda:0')\n",
      "tensor([8])   tensor([[6]], device='cuda:0')\n",
      "tensor([6])   tensor([[1]], device='cuda:0')\n",
      "tensor([5])   tensor([[8]], device='cuda:0')\n",
      "tensor([6])   tensor([[0]], device='cuda:0')\n",
      "tensor([8])   tensor([[7]], device='cuda:0')\n",
      "tensor([6])   tensor([[3]], device='cuda:0')\n",
      "tensor([6])   tensor([[5]], device='cuda:0')\n",
      "tensor([7])   tensor([[2]], device='cuda:0')\n",
      "tensor([5])   tensor([[3]], device='cuda:0')\n",
      "tensor([8])   tensor([[0]], device='cuda:0')\n",
      "tensor([7])   tensor([[1]], device='cuda:0')\n",
      "tensor([4])   tensor([[9]], device='cuda:0')\n",
      "tensor([6])   tensor([[0]], device='cuda:0')\n",
      "tensor([2])   tensor([[0]], device='cuda:0')\n",
      "tensor([0])   tensor([[6]], device='cuda:0')\n",
      "tensor([4])   tensor([[9]], device='cuda:0')\n",
      "tensor([0])   tensor([[9]], device='cuda:0')\n",
      "tensor([6])   tensor([[0]], device='cuda:0')\n",
      "tensor([9])   tensor([[5]], device='cuda:0')\n",
      "tensor([4])   tensor([[2]], device='cuda:0')\n",
      "tensor([7])   tensor([[9]], device='cuda:0')\n",
      "tensor([2])   tensor([[9]], device='cuda:0')\n",
      "tensor([3])   tensor([[7]], device='cuda:0')\n",
      "tensor([5])   tensor([[6]], device='cuda:0')\n",
      "tensor([1])   tensor([[6]], device='cuda:0')\n",
      "tensor([4])   tensor([[6]], device='cuda:0')\n",
      "tensor([8])   tensor([[0]], device='cuda:0')\n",
      "tensor([4])   tensor([[9]], device='cuda:0')\n",
      "tensor([5])   tensor([[3]], device='cuda:0')\n",
      "tensor([8])   tensor([[2]], device='cuda:0')\n",
      "tensor([8])   tensor([[2]], device='cuda:0')\n",
      "tensor([9])   tensor([[7]], device='cuda:0')\n",
      "tensor([5])   tensor([[3]], device='cuda:0')\n",
      "tensor([7])   tensor([[3]], device='cuda:0')\n",
      "tensor([4])   tensor([[6]], device='cuda:0')\n",
      "tensor([8])   tensor([[7]], device='cuda:0')\n",
      "tensor([9])   tensor([[7]], device='cuda:0')\n",
      "tensor([5])   tensor([[8]], device='cuda:0')\n",
      "tensor([5])   tensor([[6]], device='cuda:0')\n",
      "tensor([3])   tensor([[2]], device='cuda:0')\n",
      "tensor([5])   tensor([[3]], device='cuda:0')\n",
      "tensor([7])   tensor([[2]], device='cuda:0')\n",
      "tensor([6])   tensor([[1]], device='cuda:0')\n",
      "tensor([2])   tensor([[8]], device='cuda:0')\n",
      "tensor([9])   tensor([[8]], device='cuda:0')\n",
      "tensor([7])   tensor([[2]], device='cuda:0')\n",
      "tensor([3])   tensor([[5]], device='cuda:0')\n",
      "tensor([7])   tensor([[8]], device='cuda:0')\n",
      "tensor([8])   tensor([[0]], device='cuda:0')\n",
      "tensor([3])   tensor([[9]], device='cuda:0')\n",
      "tensor([9])   tensor([[3]], device='cuda:0')\n",
      "tensor([3])   tensor([[7]], device='cuda:0')\n",
      "tensor([2])   tensor([[0]], device='cuda:0')\n",
      "tensor([7])   tensor([[2]], device='cuda:0')\n",
      "tensor([9])   tensor([[7]], device='cuda:0')\n",
      "tensor([2])   tensor([[8]], device='cuda:0')\n",
      "tensor([9])   tensor([[0]], device='cuda:0')\n",
      "tensor([4])   tensor([[1]], device='cuda:0')\n",
      "tensor([8])   tensor([[7]], device='cuda:0')\n",
      "tensor([3])   tensor([[0]], device='cuda:0')\n",
      "tensor([7])   tensor([[9]], device='cuda:0')\n",
      "tensor([6])   tensor([[4]], device='cuda:0')\n",
      "tensor([3])   tensor([[5]], device='cuda:0')\n",
      "tensor([7])   tensor([[9]], device='cuda:0')\n",
      "tensor([8])   tensor([[5]], device='cuda:0')\n",
      "tensor([7])   tensor([[9]], device='cuda:0')\n",
      "tensor([9])   tensor([[4]], device='cuda:0')\n",
      "tensor([7])   tensor([[2]], device='cuda:0')\n",
      "tensor([3])   tensor([[7]], device='cuda:0')\n",
      "tensor([6])   tensor([[7]], device='cuda:0')\n",
      "tensor([9])   tensor([[5]], device='cuda:0')\n",
      "tensor([1])   tensor([[2]], device='cuda:0')\n",
      "tensor([6])   tensor([[4]], device='cuda:0')\n",
      "tensor([6])   tensor([[5]], device='cuda:0')\n",
      "tensor([3])   tensor([[5]], device='cuda:0')\n",
      "tensor([7])   tensor([[2]], device='cuda:0')\n",
      "tensor([2])   tensor([[8]], device='cuda:0')\n",
      "tensor([9])   tensor([[5]], device='cuda:0')\n",
      "tensor([9])   tensor([[7]], device='cuda:0')\n",
      "tensor([9])   tensor([[8]], device='cuda:0')\n",
      "tensor([5])   tensor([[8]], device='cuda:0')\n",
      "tensor([7])   tensor([[8]], device='cuda:0')\n",
      "tensor([7])   tensor([[1]], device='cuda:0')\n",
      "tensor([4])   tensor([[9]], device='cuda:0')\n",
      "tensor([4])   tensor([[9]], device='cuda:0')\n",
      "tensor([7])   tensor([[2]], device='cuda:0')\n",
      "tensor([9])   tensor([[7]], device='cuda:0')\n",
      "tensor([9])   tensor([[3]], device='cuda:0')\n",
      "tensor([7])   tensor([[8]], device='cuda:0')\n",
      "tensor([6])   tensor([[4]], device='cuda:0')\n",
      "tensor([5])   tensor([[8]], device='cuda:0')\n",
      "tensor([5])   tensor([[0]], device='cuda:0')\n",
      "tensor([7])   tensor([[2]], device='cuda:0')\n",
      "tensor([2])   tensor([[6]], device='cuda:0')\n",
      "tensor([0])   tensor([[7]], device='cuda:0')\n",
      "tensor([1])   tensor([[5]], device='cuda:0')\n",
      "tensor([4])   tensor([[9]], device='cuda:0')\n",
      "tensor([2])   tensor([[7]], device='cuda:0')\n",
      "tensor([1])   tensor([[7]], device='cuda:0')\n",
      "tensor([8])   tensor([[0]], device='cuda:0')\n",
      "tensor([8])   tensor([[0]], device='cuda:0')\n",
      "tensor([4])   tensor([[8]], device='cuda:0')\n",
      "tensor([4])   tensor([[0]], device='cuda:0')\n",
      "tensor([7])   tensor([[1]], device='cuda:0')\n",
      "tensor([5])   tensor([[3]], device='cuda:0')\n",
      "tensor([7])   tensor([[2]], device='cuda:0')\n",
      "tensor([8])   tensor([[9]], device='cuda:0')\n",
      "tensor([7])   tensor([[2]], device='cuda:0')\n",
      "tensor([2])   tensor([[0]], device='cuda:0')\n",
      "tensor([6])   tensor([[0]], device='cuda:0')\n",
      "tensor([3])   tensor([[8]], device='cuda:0')\n",
      "tensor([1])   tensor([[6]], device='cuda:0')\n",
      "tensor([2])   tensor([[6]], device='cuda:0')\n",
      "tensor([3])   tensor([[5]], device='cuda:0')\n",
      "tensor([7])   tensor([[3]], device='cuda:0')\n",
      "tensor([3])   tensor([[7]], device='cuda:0')\n",
      "tensor([3])   tensor([[5]], device='cuda:0')\n",
      "tensor([4])   tensor([[9]], device='cuda:0')\n",
      "tensor([8])   tensor([[7]], device='cuda:0')\n",
      "tensor([9])   tensor([[7]], device='cuda:0')\n",
      "tensor([8])   tensor([[5]], device='cuda:0')\n",
      "tensor([5])   tensor([[3]], device='cuda:0')\n",
      "tensor([1])   tensor([[8]], device='cuda:0')\n",
      "tensor([8])   tensor([[0]], device='cuda:0')\n",
      "tensor([9])   tensor([[8]], device='cuda:0')\n",
      "tensor([2])   tensor([[0]], device='cuda:0')\n",
      "tensor([2])   tensor([[7]], device='cuda:0')\n",
      "tensor([1])   tensor([[6]], device='cuda:0')\n",
      "tensor([8])   tensor([[3]], device='cuda:0')\n",
      "tensor([4])   tensor([[8]], device='cuda:0')\n",
      "tensor([7])   tensor([[3]], device='cuda:0')\n",
      "tensor([6])   tensor([[0]], device='cuda:0')\n",
      "tensor([7])   tensor([[9]], device='cuda:0')\n",
      "tensor([5])   tensor([[7]], device='cuda:0')\n",
      "tensor([9])   tensor([[5]], device='cuda:0')\n",
      "tensor([7])   tensor([[2]], device='cuda:0')\n",
      "tensor([2])   tensor([[0]], device='cuda:0')\n",
      "tensor([5])   tensor([[6]], device='cuda:0')\n",
      "tensor([4])   tensor([[6]], device='cuda:0')\n",
      "tensor([7])   tensor([[2]], device='cuda:0')\n",
      "tensor([9])   tensor([[8]], device='cuda:0')\n",
      "tensor([7])   tensor([[2]], device='cuda:0')\n",
      "tensor([4])   tensor([[6]], device='cuda:0')\n",
      "tensor([3])   tensor([[8]], device='cuda:0')\n",
      "tensor([2])   tensor([[1]], device='cuda:0')\n",
      "tensor([8])   tensor([[0]], device='cuda:0')\n",
      "tensor([7])   tensor([[2]], device='cuda:0')\n",
      "tensor([2])   tensor([[0]], device='cuda:0')\n",
      "tensor([4])   tensor([[9]], device='cuda:0')\n",
      "tensor([8])   tensor([[9]], device='cuda:0')\n",
      "tensor([8])   tensor([[9]], device='cuda:0')\n",
      "tensor([6])   tensor([[0]], device='cuda:0')\n",
      "tensor([6])   tensor([[0]], device='cuda:0')\n",
      "all:  10000 \tcorr:  9805\n"
     ]
    }
   ],
   "source": [
    "all = 0\n",
    "corr = 0\n",
    "w = SummaryWriter()\n",
    "for image, label in testDataLoard:\n",
    "    image = image.to(device)\n",
    "    out = net(image)\n",
    "    out1 = out.data.max(1, keepdim=True)[1]         #将概率值最大的数赋给out1\n",
    "    if int(label) == int(out1):\n",
    "        corr += 1\n",
    "    else:\n",
    "        #w.add_image(str(out1), image[0])\n",
    "        print(label, ' ', out1)     #打印出识别错误的图片\n",
    "    all += 1\n",
    "w.close()\n",
    "print(\"all: \", all, \"\\tcorr: \", corr)       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
